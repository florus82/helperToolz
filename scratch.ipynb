{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba025ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/potzschf/repos/')\n",
    "from helperToolz.helpsters import *\n",
    "from helperToolz.evapo import *\n",
    "from helperToolz.dicts_and_lists import INT_TO_MONTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5995743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Aldhani/users/potzschf/conda/envs/workhorse/lib/python3.12/site-packages/osgeo/ogr.py:601: FutureWarning: Neither ogr.UseExceptions() nor ogr.DontUseExceptions() has been explicitly called. In GDAL 4.0, exceptions will be enabled by default.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "files = sorted(getFilelist('/data/Aldhani/eoagritwin/et/Auxiliary/VZA/raw', '.nc'))\n",
    "year = 2020\n",
    "#get a subset of files for that year\n",
    "yearFiles = [file for file in files if int(file.split('/')[-1].split('_')[-1][0:4]) == year]\n",
    "\n",
    "# create a maks for germany\n",
    "mask = makeGermanyMaskforNC('/data/Aldhani/eoagritwin/misc/gadm41_DEU_shp/gadm41_DEU_0.shp', yearFiles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf791d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start on file /data/Aldhani/eoagritwin/et/Auxiliary/VZA/raw/Germany_2020-01-01.nc\n",
      "start on file /data/Aldhani/eoagritwin/et/Auxiliary/VZA/raw/Germany_2020-01-15.nc\n",
      "start on file /data/Aldhani/eoagritwin/et/Auxiliary/VZA/raw/Germany_2020-02-01.nc\n",
      "start on file /data/Aldhani/eoagritwin/et/Auxiliary/VZA/raw/Germany_2020-02-15.nc\n",
      "start on file /data/Aldhani/eoagritwin/et/Auxiliary/VZA/raw/Germany_2020-03-01.nc\n",
      "start on file /data/Aldhani/eoagritwin/et/Auxiliary/VZA/raw/Germany_2020-03-15.nc\n",
      "start on file /data/Aldhani/eoagritwin/et/Auxiliary/VZA/raw/Germany_2020-04-01.nc\n",
      "start on file /data/Aldhani/eoagritwin/et/Auxiliary/VZA/raw/Germany_2020-04-15.nc\n",
      "start on file /data/Aldhani/eoagritwin/et/Auxiliary/VZA/raw/Germany_2020-05-01.nc\n",
      "start on file /data/Aldhani/eoagritwin/et/Auxiliary/VZA/raw/Germany_2020-05-15.nc\n",
      "start on file /data/Aldhani/eoagritwin/et/Auxiliary/VZA/raw/Germany_2020-06-01.nc\n",
      "start on file /data/Aldhani/eoagritwin/et/Auxiliary/VZA/raw/Germany_2020-06-15.nc\n",
      "start on file /data/Aldhani/eoagritwin/et/Auxiliary/VZA/raw/Germany_2020-07-01.nc\n",
      "start on file /data/Aldhani/eoagritwin/et/Auxiliary/VZA/raw/Germany_2020-07-15.nc\n",
      "start on file /data/Aldhani/eoagritwin/et/Auxiliary/VZA/raw/Germany_2020-08-01.nc\n",
      "start on file /data/Aldhani/eoagritwin/et/Auxiliary/VZA/raw/Germany_2020-08-15.nc\n",
      "start on file /data/Aldhani/eoagritwin/et/Auxiliary/VZA/raw/Germany_2020-09-01.nc\n",
      "start on file /data/Aldhani/eoagritwin/et/Auxiliary/VZA/raw/Germany_2020-09-15.nc\n",
      "start on file /data/Aldhani/eoagritwin/et/Auxiliary/VZA/raw/Germany_2020-10-01.nc\n",
      "start on file /data/Aldhani/eoagritwin/et/Auxiliary/VZA/raw/Germany_2020-10-15.nc\n",
      "start on file /data/Aldhani/eoagritwin/et/Auxiliary/VZA/raw/Germany_2020-11-01.nc\n"
     ]
    }
   ],
   "source": [
    "tiffPath = '/data/Aldhani/eoagritwin/et/Sentinel3/tiffs/'\n",
    "yearCont = []# for collecting number of observations per year\n",
    "\n",
    "# loop over files and export to .tif at Path locations\n",
    "for i, file in enumerate(yearFiles):\n",
    "    \n",
    "    print(f'start on file {file}')\n",
    "    \n",
    "    accDateTimes = getAllDatesS3(file) # possible to take annual subset if entire files list would be passed here\n",
    "    convertNCtoTIF(file, tiffPath, file.split('/')[-1].split('.')[0] + '.tif', accDateTimes, False, True, LST=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23bf6cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncfile = file\n",
    "storPath = tiffPath\n",
    "fileName = yearFiles[0].split('/')[-1].split('.')[0] + '.tif'\n",
    "accDT=accDateTimes\n",
    "make_uint16 = False\n",
    "explode = True\n",
    "LST = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7621e813",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in [i for i in range(2017,2025,1)]:\n",
    "\n",
    "    print(f'Start processing .nc files for the year {year}')\n",
    "\n",
    "    # get a subset of files for that year\n",
    "    yearFiles = [file for file in files if int(file.split('/')[-1].split('_')[-1][0:4]) == year]\n",
    "\n",
    "    # create a maks for germany\n",
    "    mask = makeGermanyMaskforNC('/data/Aldhani/eoagritwin/misc/gadm41_DEU_shp/gadm41_DEU_0.shp', yearFiles[0])\n",
    "\n",
    "    # set storPath for exported tiffs\n",
    "    storPath = '/data/Aldhani/eoagritwin/et/Sentinel3/tiffs/'\n",
    "    LST_path = f'{storPath}LST/daily_observations_all/{year}/'\n",
    "    Time_path = f'{storPath}Acq_time/{year}/'\n",
    "    monthly_composites_path = f'{storPath}LST/monthly_composites/{year}/'\n",
    "    # ensure that directories exist\n",
    "    [os.makedirs(dir_path, exist_ok=True) for dir_path in [LST_path, Time_path, monthly_composites_path]]\n",
    "\n",
    "    yearCont = []# for collecting number of observations per year\n",
    "\n",
    "    # loop over files and export to .tif at Path locations\n",
    "    for i, file in enumerate(yearFiles):\n",
    "        \n",
    "        print(f'start on file {file}')\n",
    "        \n",
    "        accDateTimes = getAllDatesS3(file) # possible to take annual subset if entire files list would be passed here\n",
    "    #     convertNCtoTIF(file, LST_path, file.split('/')[-1].split('.')[0] + '.tif', accDateTimes, False, True)\n",
    "\n",
    "\n",
    "        dat = getDataFromNC(file)\n",
    "        monthCont = [] # for collecting number of observations per month\n",
    "        dailyCont = [] # for collecting number of observations per day\n",
    "        dailyVals_median = [] # for collection the actual daily LST values (daily median)\n",
    "        dailyVals_mean = [] # for collection the actual daily LST values (daily mean)\n",
    "        dailyVals_max = [] # for collection the actual daily LST values (daily mean)\n",
    "        bnames = []\n",
    "        df = pd.Series(accDateTimes)\n",
    "        counts_per_day = df.dt.floor(\"D\").value_counts().sort_index()\n",
    "        # vectors for indexing over days\n",
    "        cumulative_day_counts_end = np.asarray(np.cumsum(counts_per_day))\n",
    "        cumulative_day_counts_start = np.insert(cumulative_day_counts_end, 0 ,0)\n",
    "\n",
    "        # cumulative_day_counts_start = np.array(cumulative_day_counts_start)\n",
    "        # cumulative_day_counts_end = np.array(cumulative_day_counts_end)\n",
    "\n",
    "        # get accquisition times ready for aggregation and export\n",
    "        time_cube = np.full(dat.shape, None, dtype='float64')\n",
    "        for i in range(len(df)):\n",
    "            mask = ~np.isnan(dat[:,:,i])\n",
    "            time_cube[:,:,i][mask] = df[i].timestamp() # needed conversion as tif export won't work with datetimeobject\n",
    "        dailyTimeDates_max = []\n",
    "        ################################################ gives monthly min, max, median composites\n",
    "        # aggreagate by median\n",
    "        # stack_list = [\n",
    "        #     np.nanmedian(dat[:, :, start:end], axis=2)\n",
    "        #     for start, end in zip(cumulative_day_counts_start[:-1], cumulative_day_counts_end)\n",
    "        # ] \n",
    "        # fin_block = np.dstack(stack_list)\n",
    "\n",
    "        MM = INT_TO_MONTH[file.rsplit('-', maxsplit=1)[-1].split('.')[0]]\n",
    "        # bands = [f'{MM}_Day_{b+1}' for b in range(fin_block.shape[2])]\n",
    "        # fin_block = fin_block * mask[:, :, np.newaxis]\n",
    "        # fin_block[fin_block == 0] = np.nan\n",
    "        # exportNCarrayDerivatesComp(file, monthly_composites_path, f'Germany_{year}_{MM}_mean.tif', bands, fin_block)\n",
    "\n",
    "        # # aggreagate by min\n",
    "        # stack_list = [\n",
    "        #     np.nanmin(dat[:, :, start:end], axis=2)\n",
    "        #     for start, end in zip(cumulative_day_counts_start[:-1], cumulative_day_counts_end)\n",
    "        # ] \n",
    "        # fin_block = np.dstack(stack_list)\n",
    "        # fin_block = fin_block * mask[:, :, np.newaxis]\n",
    "        # fin_block[fin_block == 0] = np.nan\n",
    "        # exportNCarrayDerivatesComp(file, monthly_composites_path, f'Germany_{year}_{MM}_min.tif', bands, fin_block)\n",
    "\n",
    "        # # aggreagate by max\n",
    "        # stack_list = [\n",
    "        #     np.nanmax(dat[:, :, start:end], axis=2)\n",
    "        #     for start, end in zip(cumulative_day_counts_start[:-1], cumulative_day_counts_end)\n",
    "        # ] \n",
    "        # fin_block = np.dstack(stack_list)\n",
    "        # fin_block = fin_block * mask[:, :, np.newaxis]\n",
    "        # fin_block[fin_block == 0] = np.nan\n",
    "        # exportNCarrayDerivatesComp(file, monthly_composites_path, f'Germany_{year}_{MM}_max.tif', bands, fin_block)\n",
    "\n",
    "        ########################################################### creates metadata raster\n",
    "        for l in range(len(counts_per_day)):\n",
    "            # number of observations per month\n",
    "            monthCont.append(np.any(~np.isnan(dat[:, :, cumulative_day_counts_start[l]:cumulative_day_counts_end[l]]),axis=2)) # minimum dail obs\n",
    "            \n",
    "            # collect the dates to use as bandnames for exported tif stacks\n",
    "            bnames.append(str(counts_per_day.index[l].date()))\n",
    "\n",
    "            # collect number of observations per day ( count only one per day!)\n",
    "            dailyCont.append(np.sum(~np.isnan(dat[:, :, cumulative_day_counts_start[l]:cumulative_day_counts_end[l]]),axis=2))\n",
    "            \n",
    "            # # collect actual LST values\n",
    "            # dailyVals_median.append(np.nanmedian(dat[:, :, cumulative_day_counts_start[l]:cumulative_day_counts_end[l]], axis = 2))\n",
    "            # dailyVals_mean.append(np.nanmean(dat[:, :, cumulative_day_counts_start[l]:cumulative_day_counts_end[l]], axis = 2))\n",
    "            dailyVals_max.append(np.nanmax(dat[:, :, cumulative_day_counts_start[l]:cumulative_day_counts_end[l]], axis = 2))\n",
    "\n",
    "            # collect the acquisition time and dates in timestamp format\n",
    "            dailyTimeDates_max.append(getValsatMaxIndex(dat[:, :, cumulative_day_counts_start[l]:cumulative_day_counts_end[l]],\n",
    "                                                        time_cube[:, :, cumulative_day_counts_start[l]:cumulative_day_counts_end[l]]))\n",
    "   \n",
    "        # # export daily values\n",
    "        # exportNCarrayDerivatesInt(file, LST_path, f'Daily_LST_means_{year}_{MM}.tif', bnames, np.dstack(dailyVals_mean), make_uint16=False, numberOfBands=len(dailyVals_mean))\n",
    "        # exportNCarrayDerivatesInt(file, LST_path, f'Daily_LST_medians_{year}_{MM}.tif', bnames, np.dstack(dailyVals_median), make_uint16=False, numberOfBands=len(dailyVals_median))\n",
    "        exportNCarrayDerivatesInt(file, LST_path, f'Daily_LST_max_{year}_{MM}.tif', bnames, np.dstack(dailyVals_max), make_uint16=False, numberOfBands=len(dailyVals_max))\n",
    "        exportNCarrayDerivatesInt(file, Time_path, f'Daily_Time_max_{year}_{MM}.tif', bnames, np.dstack(dailyTimeDates_max), make_uint16=False, numberOfBands=len(dailyTimeDates_max))\n",
    "        # # export day counts\n",
    "        # exportNCarrayDerivatesInt(file, storPath + 'Analytics/Count_obs_per_day/', f'Daily_obs_for_{year}_{MM}.tif', bnames, np.dstack(dailyCont), True, numberOfBands=len(dailyCont))\n",
    "        # # export month counts\n",
    "        # exportNCarrayDerivatesInt(file, storPath + 'Analytics/Count_obs_per_month/', f'Monthly_Min_DailyObs_{('_').join(file.split('_')[-1].split('-')[:2])}.tif', 'monthly_sum_of_daily_obs', np.nansum(np.dstack((monthCont)), axis = 2), True)\n",
    "        \n",
    "        # # collect number of observations per year ( count only one per day!)\n",
    "        # yearCont.append(np.nansum(np.dstack((monthCont)), axis = 2))\n",
    "\n",
    "    # # export year counts\n",
    "    # exportNCarrayDerivatesInt(file, storPath + 'Analytics/Count_obs_per_year/', f'Annual_Min_DailyObs_{file.split('_')[-1].split('-')[0]}.tif', 'annual_sum_of_daily_obs', np.nansum(np.dstack((yearCont)), axis = 2), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc71c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBluGrnRedBnrFORCEList(filelist):\n",
    "    '''Takes a list of paths to an exploded FORCE output and returns a list with ordered paths\n",
    "    First all blue then green, red and bnir bands. Furthermore, paths are chronologically sorted (1,2,3,4..months)'''\n",
    "    blu = [file for file in filelist if file.split('SEN2H_')[-1].split('_')[0] == 'BLU']\n",
    "    grn = [file for file in filelist if file.split('SEN2H_')[-1].split('_')[0] == 'GRN']\n",
    "    red = [file for file in filelist if file.split('SEN2H_')[-1].split('_')[0] == 'RED']\n",
    "    bnr = [file for file in filelist if file.split('SEN2H_')[-1].split('_')[0] == 'BNR']\n",
    "\n",
    "    blu = sortListwithOtherlist([int(t.split('-')[-1].split('.')[0]) for t in blu], blu)[-1]\n",
    "    grn = sortListwithOtherlist([int(t.split('-')[-1].split('.')[0]) for t in grn], grn)[-1]\n",
    "    red = sortListwithOtherlist([int(t.split('-')[-1].split('.')[0]) for t in red], red)[-1]\n",
    "    bnr = sortListwithOtherlist([int(t.split('-')[-1].split('.')[0]) for t in bnr], bnr)[-1]\n",
    "\n",
    "    return sum([blu, grn, red, bnr], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fc0426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getExplodedFORCEbyColorAndTime(fileList, colorList, timeList=None):\n",
    "    \"\"\"This function help to order paths of FORCEfiles according to a list of color, e.g. ['BLU', 'GRN',...]\n",
    "    Furthermore, output will be sorted against a list of time, if provided\n",
    "\n",
    "    Args:\n",
    "        fileList (list): a list of paths to exploded FORCE output\n",
    "        colorList (list): a list of the colors to group the output by\n",
    "    \"\"\"\n",
    "\n",
    "color_pattern = rf'_{target_color}_'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd8cf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileList=['/data/Aldhani/eoagritwin/force/output/Guzinski/1TileTest/X0068_Y0042/20190101-20191231_060-310_HL_TSA_SEN2L_BLU_TSI_20190630.tif',\n",
    " '/data/Aldhani/eoagritwin/force/output/Guzinski/1TileTest/X0067_Y0042/20190101-20191231_001-365_HL_TSA_SEN2L_BNR_TSI_20190630.tif',\n",
    " '/data/Aldhani/eoagritwin/force/output/Guzinski/1TileTest/X0068_Y0042/20190101-20191231_060-310_HL_TSA_SEN2L_BNR_TSI_20190630.tif',\n",
    " '/data/Aldhani/eoagritwin/force/output/Guzinski/1TileTest/X0067_Y0042/20190101-20191231_001-365_HL_TSA_SEN2L_GRN_TSI_20190630.tif',\n",
    " '/data/Aldhani/eoagritwin/force/output/Guzinski/1TileTest/X0068_Y0042/20190101-20191231_060-310_HL_TSA_SEN2L_GRN_TSI_20190630.tif',\n",
    " '/data/Aldhani/eoagritwin/force/output/Guzinski/1TileTest/X0067_Y0042/20190101-20191231_001-365_HL_TSA_SEN2L_RE1_TSI_20190630.tif']\n",
    "\n",
    "colorList=['BNR']\n",
    "\n",
    "for target_color in colorList:\n",
    "    search_pattern = rf'_{target_color}_'\n",
    "\n",
    "    print([file for file in fileList if re.search(search_pattern, file)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce12996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a =random.randint(0,9999)\n",
    "print(f'{a:04d}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workhorse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
